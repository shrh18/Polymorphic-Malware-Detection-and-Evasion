{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":872432,"sourceType":"datasetVersion","datasetId":464228}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport glob\nimport os\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\npd.set_option('display.max_columns', None, 'max_colwidth', None, 'display.expand_frame_repr', False)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T04:38:35.055777Z","iopub.execute_input":"2023-11-24T04:38:35.056155Z","iopub.status.idle":"2023-11-24T04:38:36.015992Z","shell.execute_reply.started":"2023-11-24T04:38:35.056125Z","shell.execute_reply":"2023-11-24T04:38:36.014941Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport glob\n# get file paths of all csv files in the folder\ncsv_files = glob.glob('/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/*.csv')\n\n# read csv files and store them in a list of dataframes\ndfs = [pd.read_csv(file) for file in csv_files]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-24T04:38:43.475532Z","iopub.execute_input":"2023-11-24T04:38:43.476056Z","iopub.status.idle":"2023-11-24T04:39:05.171181Z","shell.execute_reply.started":"2023-11-24T04:38:43.476024Z","shell.execute_reply":"2023-11-24T04:39:05.169473Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.concat(dfs)\n#print(' Label' in df.columns)\n\ndef clean_column_name(column):\n    column = column.strip(' ')\n    column = column.replace('/', '_')\n    column = column.replace(' ', '_')\n    column = column.lower()\n    return column\n\n\ndf.columns = [clean_column_name(column) for column in df.columns]\ndf.drop(columns=['fwd_header_length.1'], inplace=True)\ndf.replace([-np.inf, np.inf], np.nan, inplace=True)\ndf.drop_duplicates(inplace=True, keep=False, ignore_index=True)\ndf.dropna(axis=0, inplace=True, how=\"any\")\nprint(df.shape)\nbenign_rows = df[df['label'] == 'BENIGN']\nprint(len(benign_rows))\nprint(df.shape[0] - len(benign_rows))\n\ndf[['flow_bytes_s', 'flow_packets_s']] = df[['flow_bytes_s', 'flow_packets_s']].apply(pd.to_numeric)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-24T04:39:12.628205Z","iopub.execute_input":"2023-11-24T04:39:12.628581Z","iopub.status.idle":"2023-11-24T04:39:30.894616Z","shell.execute_reply.started":"2023-11-24T04:39:12.628550Z","shell.execute_reply":"2023-11-24T04:39:30.892736Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(2425727, 78)\n2035505\n390222\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import OneHotEncoder\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.under_sampling import RandomUnderSampler\n\nlabels = df['label']\n# for(label in labels):\n    \ncategorical_features = df.select_dtypes(include=['object']).columns\nfeatures = df.drop('label', axis=1)\nnumeric_features = features.select_dtypes(exclude=[object]).columns\n\n# # separate benign and non-benign rows\ndf_benign = df[df['label'] == 'BENIGN']\ndf_non_benign = df[df['label'] != 'BENIGN']\ntotal_samples_size = 2425727\nbenign_sample_size = total_samples_size - len(df_non_benign)\n\nsampled_benign = df_benign.sample(n=benign_sample_size, random_state=42)\nsampled_non_benign = df_non_benign.sample(n=total_samples_size-benign_sample_size, random_state=42)\n\ndf_balanced = pd.concat([sampled_benign, sampled_non_benign])\ndf_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\nencoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='error')\n\n# fit and transform the labels\none_hot_labels = encoder.fit_transform(labels.values.reshape(-1, 1))\ndf_balanced['label']\n\n\n# # split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(df_balanced.drop('label', axis=1), one_hot_labels, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-24T04:43:59.584942Z","iopub.execute_input":"2023-11-24T04:43:59.585333Z","iopub.status.idle":"2023-11-24T04:44:10.173532Z","shell.execute_reply.started":"2023-11-24T04:43:59.585303Z","shell.execute_reply":"2023-11-24T04:44:10.171910Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(\"Data type of labels:\", y_train.dtype)\ny\n# y_train = y_train.astype(int)\n# print(\"Data type of labels:\", y_train.dtype)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T04:44:20.326701Z","iopub.execute_input":"2023-11-24T04:44:20.327094Z","iopub.status.idle":"2023-11-24T04:44:20.354228Z","shell.execute_reply.started":"2023-11-24T04:44:20.327063Z","shell.execute_reply":"2023-11-24T04:44:20.352098Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Data type of labels: float64\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData type of labels:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m----> 2\u001b[0m \u001b[43my\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# y_train = y_train.astype(int)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(\"Data type of labels:\", y_train.dtype)\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"],"ename":"NameError","evalue":"name 'y' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Assuming X_train is your input data\nnum_features = X_train.shape[1]\nBATCH_SIZE = 256\nnoise_dim = 100\n\n# Data preprocessing\nscaler = MinMaxScaler(feature_range=(-1, 1))\nX_train_scaled = scaler.fit_transform(X_train)\ntrain_dataset = tf.data.Dataset.from_tensor_slices(X_train_scaled).shuffle(len(X_train_scaled)).batch(BATCH_SIZE)\n\n# Generator Model\ndef make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(256, use_bias=False, input_shape=(noise_dim,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Dense(512, use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Dense(num_features, use_bias=False, activation='tanh'))\n    return model\n\n# Discriminator Model\ndef make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(512, input_shape=(num_features,)))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Dense(256))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Dense(1))\n    return model\n\n# Loss functions\ndef discriminator_loss(real_output, fake_output):\n    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n\ndef generator_loss(fake_output):\n    return -tf.reduce_mean(fake_output)\n\n# Create the models\ngenerator = make_generator_model()\ndiscriminator = make_discriminator_model()\n\n# Optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n# Training step\n@tf.function\ndef train_step(batch_data):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_data = generator(noise, training=True)\n\n        real_output = discriminator(batch_data, training=True)\n        fake_output = discriminator(generated_data, training=True)\n\n        disc_loss = discriminator_loss(real_output, fake_output)\n        gen_loss = generator_loss(fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n\n    return gen_loss, disc_loss\n\n# Training process\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        epoch_gen_loss = 0\n        epoch_disc_loss = 0\n        num_batches = 0\n\n        for batch_data in dataset:\n            gen_loss, disc_loss = train_step(batch_data)\n            epoch_gen_loss += gen_loss\n            epoch_disc_loss += disc_loss\n            num_batches += 1\n\n        epoch_gen_loss /= num_batches\n        epoch_disc_loss /= num_batches\n\n        print(f\"Epoch {epoch+1}/{epochs} - Generator Loss: {epoch_gen_loss:.4f}, Discriminator Loss: {epoch_disc_loss:.4f}\")\n\n# Start training\nEPOCHS = 50\ntrain(train_dataset, EPOCHS)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-24T05:21:14.452707Z","iopub.execute_input":"2023-11-24T05:21:14.453138Z","iopub.status.idle":"2023-11-24T07:03:49.978185Z","shell.execute_reply.started":"2023-11-24T05:21:14.453105Z","shell.execute_reply":"2023-11-24T07:03:49.975206Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/50 - Generator Loss: -154.3338, Discriminator Loss: -14.2048\nEpoch 2/50 - Generator Loss: -675.2687, Discriminator Loss: -44.9583\nEpoch 3/50 - Generator Loss: -1077.7354, Discriminator Loss: -68.5356\nEpoch 4/50 - Generator Loss: -2087.0396, Discriminator Loss: -131.6456\nEpoch 5/50 - Generator Loss: -3334.7212, Discriminator Loss: -176.2631\nEpoch 6/50 - Generator Loss: -5009.2783, Discriminator Loss: -238.7819\nEpoch 7/50 - Generator Loss: -5576.6147, Discriminator Loss: -244.3985\nEpoch 8/50 - Generator Loss: -6048.3818, Discriminator Loss: -307.4615\nEpoch 9/50 - Generator Loss: -5510.9995, Discriminator Loss: -368.2141\nEpoch 10/50 - Generator Loss: -9254.1514, Discriminator Loss: -401.0631\nEpoch 11/50 - Generator Loss: -15922.7490, Discriminator Loss: -457.7238\nEpoch 12/50 - Generator Loss: -18908.7871, Discriminator Loss: -340.6344\nEpoch 13/50 - Generator Loss: -17526.5449, Discriminator Loss: -304.8133\nEpoch 14/50 - Generator Loss: -17409.3438, Discriminator Loss: -322.0965\nEpoch 15/50 - Generator Loss: -19844.1836, Discriminator Loss: -293.6898\nEpoch 16/50 - Generator Loss: -19236.2441, Discriminator Loss: -272.1252\nEpoch 17/50 - Generator Loss: -18843.7520, Discriminator Loss: -294.4409\nEpoch 18/50 - Generator Loss: -20771.4414, Discriminator Loss: -286.1066\nEpoch 19/50 - Generator Loss: -21159.9922, Discriminator Loss: -330.9266\nEpoch 20/50 - Generator Loss: -22143.1797, Discriminator Loss: -341.9296\nEpoch 21/50 - Generator Loss: -23223.0938, Discriminator Loss: -375.3036\nEpoch 22/50 - Generator Loss: -26212.5625, Discriminator Loss: -411.2763\nEpoch 23/50 - Generator Loss: -26906.8789, Discriminator Loss: -430.1822\nEpoch 24/50 - Generator Loss: -27000.4336, Discriminator Loss: -472.6002\nEpoch 25/50 - Generator Loss: -29589.0645, Discriminator Loss: -744.8215\nEpoch 26/50 - Generator Loss: -35061.1758, Discriminator Loss: -471.7504\nEpoch 27/50 - Generator Loss: -36551.1719, Discriminator Loss: -711.1407\nEpoch 28/50 - Generator Loss: -41512.6211, Discriminator Loss: -490.1486\nEpoch 29/50 - Generator Loss: -44614.5625, Discriminator Loss: -518.4832\nEpoch 30/50 - Generator Loss: -44185.2109, Discriminator Loss: -536.1170\nEpoch 31/50 - Generator Loss: -46529.8398, Discriminator Loss: -563.2372\nEpoch 32/50 - Generator Loss: -50320.6992, Discriminator Loss: -596.8764\nEpoch 33/50 - Generator Loss: -51935.2383, Discriminator Loss: -931.4933\nEpoch 34/50 - Generator Loss: -58093.5508, Discriminator Loss: -526.4993\nEpoch 35/50 - Generator Loss: -58398.1133, Discriminator Loss: -579.9780\nEpoch 36/50 - Generator Loss: -60137.3945, Discriminator Loss: -606.5105\nEpoch 37/50 - Generator Loss: -59673.8945, Discriminator Loss: -642.7852\nEpoch 38/50 - Generator Loss: -64588.1953, Discriminator Loss: -679.7801\nEpoch 39/50 - Generator Loss: -63943.7461, Discriminator Loss: -768.4067\nEpoch 40/50 - Generator Loss: -68470.0703, Discriminator Loss: -739.3762\nEpoch 41/50 - Generator Loss: -69989.3125, Discriminator Loss: -1183.9062\nEpoch 42/50 - Generator Loss: -75184.7812, Discriminator Loss: -681.0536\nEpoch 43/50 - Generator Loss: -76231.5469, Discriminator Loss: -778.8708\nEpoch 44/50 - Generator Loss: -79317.4062, Discriminator Loss: -817.9098\nEpoch 45/50 - Generator Loss: -78282.9219, Discriminator Loss: -838.0082\nEpoch 46/50 - Generator Loss: -79676.7812, Discriminator Loss: -876.0604\nEpoch 47/50 - Generator Loss: -80808.0859, Discriminator Loss: -935.0748\nEpoch 48/50 - Generator Loss: -81599.7656, Discriminator Loss: -997.0228\nEpoch 49/50 - Generator Loss: -83183.9844, Discriminator Loss: -1015.3306\nEpoch 50/50 - Generator Loss: -89314.0703, Discriminator Loss: -994.1266\n","output_type":"stream"}]},{"cell_type":"code","source":"X_test_scaled = scaler.transform(X_test)\nX_test_dataset = tf.data.Dataset.from_tensor_slices(X_test_scaled).shuffle(len(X_test_scaled)).batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T07:08:51.315326Z","iopub.execute_input":"2023-11-24T07:08:51.316488Z","iopub.status.idle":"2023-11-24T07:08:51.878015Z","shell.execute_reply.started":"2023-11-24T07:08:51.316422Z","shell.execute_reply":"2023-11-24T07:08:51.876847Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def evaluate_discriminator(test_dataset, generator, discriminator):\n    real_count, fake_count = 0, 0\n\n    for test_batch in test_dataset:\n        # Test discriminator on real data\n        real_output = discriminator(test_batch, training=False)\n        real_count += np.sum(real_output.numpy() > 0.5)  # Count how many are classified as real\n\n        # Generate fake data and test discriminator\n        noise = tf.random.normal([BATCH_SIZE, noise_dim])\n        fake_data = generator(noise, training=False)\n        fake_output = discriminator(fake_data, training=False)\n        fake_count += np.sum(fake_output.numpy() <= 0.5)  # Count how many are classified as fake\n\n    return real_count, fake_count\n\nreal_count, fake_count = evaluate_discriminator(X_test_dataset, generator, discriminator)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T07:09:03.160750Z","iopub.execute_input":"2023-11-24T07:09:03.161102Z","iopub.status.idle":"2023-11-24T07:09:43.534882Z","shell.execute_reply.started":"2023-11-24T07:09:03.161077Z","shell.execute_reply":"2023-11-24T07:09:43.533247Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"real_count, fake_count","metadata":{"execution":{"iopub.status.busy":"2023-11-24T07:28:07.238088Z","iopub.execute_input":"2023-11-24T07:28:07.238523Z","iopub.status.idle":"2023-11-24T07:28:07.252746Z","shell.execute_reply.started":"2023-11-24T07:28:07.238492Z","shell.execute_reply":"2023-11-24T07:28:07.251224Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(485146, 0)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}